name: Update Options Data

on:
  schedule:
    # Run at 3 AM UTC on weekdays (Monday to Friday)
    - cron: '0 3 * * 1-5'
  workflow_dispatch: # Allow manual triggering
    inputs:
      target_date:
        description: 'Target date for data extraction (YYYY-MM-DD). Defaults to current date.'
        required: false
        type: string
      force_fresh_clone:
        description: 'Force fresh clone instead of using cache'
        required: false
        type: boolean
        default: false

env:
  DOLT_DIR: 'dolt_workspace'
  DATABASE_DIR: 'options'
  OUTPUT_DIR: 'data_output'
  DATA_CSV: 'options_data.csv'
  DATES_CSV: 'latest_date.csv'
  API_DIR: 'api'
  SYMBOLS_JSON: 'symbols.json'
  METADATA_JSON: 'metadata.json'
  INDIV_SYMBOLS_DIR: 'symbols'

  # vars.NODE_VERSION: "22"
  # vars.DOLT_REPO_URL: post-no-preference/options
  # vars.DOLT_DATABASE: option_chain
  # vars.NPM_CMD: process-data

  # github.event.inputs.target_date
  # github.event.inputs.force_fresh_clone

jobs:
  # Job 1: Setup and data extraction
  extract-data:
    runs-on: ubuntu-latest
    outputs:
      actual-date: ${{ steps.extract.outputs.actual-date }}
      data-available: ${{ steps.extract.outputs.data-available }}
      cache-hit: ${{ steps.cache-database.outputs.cache-hit }}
      source-changed: ${{ steps.check-source.outputs.source-changed }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Cache Dolt installation
        id: cache-dolt
        uses: actions/cache@v4
        with:
          path: $HOME/.dolt
          key: dolt-latest-${{ runner.os }}
          restore-keys: |
            dolt-latest-
            dolt-

      - name: Install Dolt
        if: steps.cache-dolt.outputs.cache-hit != 'true'
        run: |
          sudo bash -c 'curl -L https://github.com/dolthub/dolt/releases/latest/download/install.sh | bash'

      - name: Add Dolt to PATH
        run: echo "$HOME/.dolt/bin" >> $GITHUB_PATH

      - name: Verify Dolt is now in PATH
        run: |
          dolt version
          which dolt

      - name: Cache Dolt database
        id: cache-database
        uses: actions/cache@v4
        with:
          path: ${{ env.DOLT_DIR }}/${{ env.DATABASE_DIR }}
          key: dolt-options-db-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            dolt-options-db-

      - name: Set target date
        run: |
          if [ -n "${{ github.event.inputs.target_date }}" ]; then
            echo "TARGET_DATE=${{ github.event.inputs.target_date }}" >> $GITHUB_ENV
          else
            echo "TARGET_DATE=$(date -u +%Y-%m-%d)" >> $GITHUB_ENV
          fi

      - name: Create directory for Dolt operations
        run: |
          mkdir -p ${{ env.DOLT_DIR }}

      - name: Clone Dolt database
        working-directory: ${{ env.DOLT_DIR }}
        run: |
          if [ "${{ github.event.inputs.force_fresh_clone }}" == "true" ] || [ ! -d "${{ env.DATABASE_DIR }}/.dolt" ]; then
            echo "Performing fresh clone of dolt database..."
            rm -rf ${{ env.DATABASE_DIR }}
            dolt clone ${{ vars.DOLT_REPO_URL }} ${{ env.DATABASE_DIR }}
            cd ${{ env.DATABASE_DIR }}
            echo "Fresh clone completed"
          else
            echo "Using cached database, pulling latest changes..."
            cd ${{ env.DATABASE_DIR }}
            
            # Configure remote if not already set
            if ! dolt remote -v | grep -q "origin"; then
              dolt remote add origin ${{ vars.DOLT_REPO_URL }}
            fi
            
            # Pull latest changes
            dolt pull origin master || {
              echo "Pull failed, falling back to fresh clone..."
              cd ..
              rm -rf ${{ env.DATABASE_DIR }}
              dolt clone ${{ vars.DOLT_REPO_URL }} ${{ env.DATABASE_DIR }}
              cd ${{ env.DATABASE_DIR }}
            }
          fi

          # Verify database is working
          dolt sql -q "SHOW TABLES;" || {
            echo "Database verification failed, attempting fresh clone..."
            cd ..
            rm -rf ${{ env.DATABASE_DIR }}
            dolt clone ${{ vars.DOLT_REPO_URL }} ${{ env.DATABASE_DIR }}
            cd ${{ env.DATABASE_DIR }}
            dolt sql -q "SHOW TABLES;"
          }

      - name: Check if update needed
        id: check-update
        run: |
          # Check if we already have data for this date
          if [ -f "./${{ env.API_DIR }}/${{ env.METADATA_JSON }}" ]; then
            EXISTING_DATE=$(jq -r '.dataDate' ./${{ env.API_DIR }}/${{ env.METADATA_JSON }})
            if [ "$EXISTING_DATE" = "$TARGET_DATE" ]; then
              echo "Data already up to date for $TARGET_DATE"
              echo "skip-update=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          echo "skip-update=false" >> $GITHUB_OUTPUT

      - name: Extract options data
        id: extract
        if: steps.check-update.outputs.skip-update != 'true'
        working-directory: ${{ env.DOLT_DIR }}/${{ env.DATABASE_DIR }}
        run: |
          echo "Extracting data for date: $TARGET_DATE"

          # Create output directory
          mkdir -p ../../${{ env.OUTPUT_DIR }}

          # Cache data csv path
          DATA_CSV_PATH="${{ env.OUTPUT_DIR }}/${{ env.DATA_CSV }}"

          # First check if data exists for target date (fast query)
          DATA_EXISTS=$(dolt sql -q "SELECT COUNT(*) as count FROM ${{ vars.DOLT_DATABASE }} WHERE date = '$TARGET_DATE' AND (bid > 0 OR ask > 0) LIMIT 1" -r csv | tail -n 1)

          if [ "$DATA_EXISTS" -gt 0 ]; then
            QUERY_DATE="$TARGET_DATE"
            echo "actual-date=$TARGET_DATE" >> $GITHUB_OUTPUT
          else
            # Find latest date efficiently with index
            LATEST_DATE=$(dolt sql -q "
              SELECT date 
              FROM ${{ vars.DOLT_DATABASE }} 
              WHERE date >= DATE_SUB('$TARGET_DATE', INTERVAL 7 DAY) 
                AND date <= '$TARGET_DATE'
                AND (bid > 0 OR ask > 0)
              ORDER BY date DESC 
              LIMIT 1
            " -r csv | tail -n 1)
            
            if [ -z "$LATEST_DATE" ]; then
              echo "No recent data found"
              echo "data-available=false" >> $GITHUB_OUTPUT
              exit 1
            fi
            
            QUERY_DATE="$LATEST_DATE"
            echo "actual-date=$LATEST_DATE" >> $GITHUB_OUTPUT
          fi

          # Export data to CSV for processing
          dolt sql -q "
            SELECT /*+ USE_INDEX(date_idx) */ 
              date, act_symbol, expiration, strike, call_put,
              bid, ask, vol, delta, gamma, theta, vega, rho
            FROM ${{ vars.DOLT_DATABASE }} 
            WHERE date = '$QUERY_DATE' 
              AND (bid > 0 OR ask > 0)
            ORDER BY act_symbol, expiration, call_put, strike
          " -r csv > ../../"$DATA_CSV_PATH"

          echo "data-available=true" >> $GITHUB_OUTPUT
          # Verify we have data
          echo "Data rows extracted: $(( $(wc -l < ../../"$DATA_CSV_PATH") - 1 ))"

      - name: Database maintenance
        working-directory: ${{ env.DOLT_DIR }}/${{ env.DATABASE_DIR }}
        run: |
          echo "Performing database maintenance..."
          dolt status
          dolt reset --hard HEAD || true

      - name: Upload extracted data
        uses: actions/upload-artifact@v4
        with:
          name: extracted-data
          path: ${{ env.OUTPUT_DIR }}
          retention-days: 1
    
      - name: Check source code changes
        id: check-source
        run: |
          # Check if TypeScript source files changed in last commit
          if git diff --quiet HEAD~1 HEAD -- src/ tsconfig.json package.json package-lock.json 2>/dev/null; then
            echo "source-changed=false" >> $GITHUB_OUTPUT
            echo "No source code changes detected"
          else
            echo "source-changed=true" >> $GITHUB_OUTPUT
            echo "Source code changes detected"
          fi
        continue-on-error: true  # Don't fail on first run or git errors

  # Job 2: Build TypeScript
  build-typescript:
    runs-on: ubuntu-latest
    needs: [extract-data]
    if: needs.extract-data.outputs.source-changed != 'false'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Cache TypeScript build
        id: cache-ts-build
        uses: actions/cache@v4
        with:
          path: |
            dist/
            node_modules/.cache/
          key: ts-build-${{ hashFiles('src/**/*.ts', 'src/**/*.js', 'tsconfig.json', 'package.json') }}
          restore-keys: |
            ts-build-${{ hashFiles('src/**/*.ts', 'src/**/*.js', 'tsconfig.json') }}-
            ts-build-

      - name: Check build skip
        id: build-skip
        run: |
          if [ "${{ steps.cache-ts-build.outputs.cache-hit }}" = "true" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "Using cached TypeScript build"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
            echo "Building TypeScript from source"
          fi

      - name: Setup Node.js
        if: steps.build-skip.outputs.skip != 'true'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ vars.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        if: steps.build-skip.outputs.skip != 'true'
        run: npm ci

      - name: Build TypeScript project
        if: steps.build-skip.outputs.skip != 'true'
        run: npm run build

      - name: Prepare build artifacts
        run: |
          if [ "${{ steps.build-skip.outputs.skip }}" = "true" ]; then
            echo "Preparing cached artifacts for upload"
            # Ensure package files exist for next job
            test -f package.json && test -f package-lock.json
          fi

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: typescript-build
          path: |
            dist/
            node_modules/
            package.json
            package-lock.json
          retention-days: 1

  # Job 3: Process data and generate API files
  process-data:
    runs-on: ubuntu-latest
    needs: [extract-data, build-typescript]
    if: needs.extract-data.outputs.data-available == 'true'
    outputs:
      symbol-count: ${{ steps.process.outputs.symbol-count }}
      contract-count: ${{ steps.process.outputs.contract-count }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download extracted data
        uses: actions/download-artifact@v4
        with:
          name: extracted-data
          path: ${{ env.OUTPUT_DIR }}

      - name: Download TypeScript build
        uses: actions/download-artifact@v4
        with:
          name: typescript-build
          path: .

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ vars.NODE_VERSION }}

      - name: Process data with TypeScript
        id: process
        run: |
          echo "Processing options data with streaming..."
          
          # Set Node.js memory limits for large datasets
          export NODE_OPTIONS="--max-old-space-size=4096"
          
          # Enable worker threads for parallel processing if available
          export UV_THREADPOOL_SIZE=8
          
          # Run the main data processing script
          npm run ${{ vars.NPM_CMD }}
          
          # Extract statistics for outputs
          METADATA_JSON_PATH="./${{ env.API_DIR }}/${{ env.METADATA_JSON }}"
          if [ -f "$METADATA_JSON_PATH" ]; then
            SYMBOL_COUNT=$(jq -r '.symbolCount' "$METADATA_JSON_PATH")
            CONTRACT_COUNT=$(jq -r '.totalContracts' "$METADATA_JSON_PATH")
            echo "symbol-count=$SYMBOL_COUNT" >> $GITHUB_OUTPUT
            echo "contract-count=$CONTRACT_COUNT" >> $GITHUB_OUTPUT
          fi
        env:
          DATA_INPUT_PATH: ./${{ env.OUTPUT_DIR }}/${{ env.DATA_CSV }}
          API_OUTPUT_PATH: ./${{ env.API_DIR }}

      - name: Validate generated API files
        run: |
          echo "Validating generated files..."

          # Cache JSON path and symbols path
          SYMBOLS_JSON_PATH="./${{ env.API_DIR }}/${{ env.SYMBOLS_JSON }}"
          METADATA_JSON_PATH="./${{ env.API_DIR }}/${{ env.METADATA_JSON }}"
          SYMBOLS_PATH="./${{ env.API_DIR }}/${{ env.INDIV_SYMBOLS_DIR }}"

          # Check if required files exist
          if [ ! -f "$SYMBOLS_JSON_PATH" ]; then
            echo "Error: ${{ env.SYMBOLS_JSON }} not generated"
            exit 1
          fi

          if [ ! -f "$METADATA_JSON_PATH" ]; then
            echo "Error: ${{ env.METADATA_JSON }} not generated"
            exit 1
          fi

          # Check if we have symbol files
          SYMBOL_COUNT=$(find "$SYMBOLS_PATH" -name "*.json" 2>/dev/null | wc -l)
          if [ "$SYMBOL_COUNT" -eq 0 ]; then
            echo "Error: No symbol files generated"
            exit 1
          fi

          echo "Generated $SYMBOL_COUNT symbol files"

          # Validate JSON structure
          echo "Validating JSON files..."
          if ! jq empty "$SYMBOLS_JSON_PATH" 2>/dev/null; then
            echo "Error: ${{ env.SYMBOLS_JSON }} is not valid JSON"
            exit 1
          fi

          if ! jq empty "$METADATA_JSON_PATH" 2>/dev/null; then
            echo "Error: ${{ env.METADATA_JSON }} is not valid JSON"
            exit 1
          fi

          echo "All validations passed"

      - name: Upload processed API files
        uses: actions/upload-artifact@v4
        with:
          name: api-files
          path: ${{ env.API_DIR }}
          retention-days: 1

  # Job 4: Generate documentation (parallel with data processing)
  generate-docs:
    runs-on: ubuntu-latest
    needs: [extract-data]
    if: needs.extract-data.outputs.data-available == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate API documentation
        run: |
          echo "Generating API documentation..."

          # Create API directory
          mkdir -p ./${{ env.API_DIR }}

          # Define repository url
          REPO_URL="https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"

          # Cache JSON path and symbols path
          SYMBOLS_JSON_FILE="${{ env.SYMBOLS_JSON }}"
          METADATA_JSON_FILE="${{ env.METADATA_JSON }}" 
          SYMBOLS_DIR="${{ env.INDIV_SYMBOLS_DIR }}"

          # Generate documentation
          cat > ./${{ env.API_DIR }}/index.html << EOF
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <title>Options Chain API</title>
              <style>
                  body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
                  code { background: #f4f4f4; padding: 2px 4px; border-radius: 3px; }
                  pre { background: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }
                  .endpoint { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
                  .method { color: #2e8b57; font-weight: bold; }
              </style>
          </head>
          <body>
              <h1>Options Chain API</h1>
              <p>Free options chain data API updated daily on weekdays.</p>
              
              <div class="endpoint">
                  <h3><span class="method">GET</span> /$SYMBOLS_JSON_FILE</h3>
                  <p>Returns a list of all available symbols.</p>
                  <pre><code>curl $REPO_URL/$SYMBOLS_JSON_FILE</code></pre>
              </div>
              
              <div class="endpoint">
                  <h3><span class="method">GET</span> /$SYMBOLS_DIR/{SYMBOL}.json</h3>
                  <p>Returns all option contracts for a specific symbol.</p>
                  <pre><code>curl $REPO_URL/$SYMBOLS_DIR/AAPL.json</code></pre>
              </div>
              
              <div class="endpoint">
                  <h3><span class="method">GET</span> /$METADATA_JSON_FILE</h3>
                  <p>Returns metadata about the dataset including counts and last update time.</p>
                  <pre><code>curl $REPO_URL/$METADATA_JSON_FILE</code></pre>
              </div>
              
              <h3>Data Structure</h3>
              <p>Each contract contains the following fields:</p>
              <ul>
                  <li><code>date</code> - Data date</li>
                  <li><code>symbol</code> - Underlying symbol</li>
                  <li><code>expiration</code> - Option expiration date</li>
                  <li><code>strike</code> - Strike price</li>
                  <li><code>type</code> - 'call' or 'put'</li>
                  <li><code>bid</code> - Bid price</li>
                  <li><code>ask</code> - Ask price</li>
                  <li><code>volume</code> - Trading volume</li>
                  <li><code>delta</code> - Delta Greek</li>
                  <li><code>gamma</code> - Gamma Greek</li>
                  <li><code>theta</code> - Theta Greek</li>
                  <li><code>vega</code> - Vega Greek</li>
                  <li><code>rho</code> - Rho Greek</li>
              </ul>
              
              <h3>Rate Limits</h3>
              <p>This API is served via GitHub Pages and subject to GitHub's usage policies. Please be respectful with request frequency.</p>
              
              <h3>Data Source</h3>
              <p>Data is updated daily on weekdays at 3 AM UTC using the ${{ vars.DOLT_REPO_URL }} Dolt database.</p>
              
              <p><strong>Last Updated:</strong> <span id="lastUpdated">Loading...</span></p>
              
              <script>
                  fetch('./$METADATA_JSON_FILE')
                      .then(response => response.json())
                      .then(data => {
                          document.getElementById('lastUpdated').textContent = 
                              new Date(data.lastUpdated).toLocaleString();
                      })
                      .catch(() => {
                          document.getElementById('lastUpdated').textContent = 'Unknown';
                      });
              </script>
          </body>
          </html>
          EOF

      - name: Upload documentation
        uses: actions/upload-artifact@v4
        with:
          name: documentation
          path: ${{ env.API_DIR }}/index.html
          retention-days: 1

  # Job 5: Deploy and finalize (depends on all previous jobs)
  deploy:
    runs-on: ubuntu-latest
    environment: github-pages
    needs: [extract-data, process-data, generate-docs]
    if: needs.extract-data.outputs.data-available == 'true'

    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download API files
        uses: actions/download-artifact@v4
        with:
          name: api-files
          path: ${{ env.API_DIR }}

      - name: Download documentation
        uses: actions/download-artifact@v4
        with:
          name: documentation
          path: ./temp-docs

      - name: Merge documentation
        run: |
          cp ./temp-docs/index.html ./${{ env.API_DIR }}/
          rm -rf ./temp-docs

      - name: Update README with statistics
        run: |
          echo "Updating README with latest statistics..."

          # Define repository url
          REPO_URL="https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}"

          # Cache metadata JSON path
          METADATA_JSON_PATH="${{ env.API_DIR }}/${{ env.METADATA_JSON }}"

          # Extract statistics from metadata
          SYMBOL_COUNT=$(jq -r '.symbolCount' ./"$METADATA_JSON_PATH")
          CONTRACT_COUNT=$(jq -r '.totalContracts' ./"$METADATA_JSON_PATH")
          LAST_UPDATED=$(jq -r '.lastUpdated' ./"$METADATA_JSON_PATH")
          DATA_DATE=$(jq -r '.dataDate' ./"$METADATA_JSON_PATH")

          # Update README
          cat > README.md << EOF
          # Options Chain API

          A free, daily-updated options chain data API served via GitHub Pages.

          ## üìä Current Statistics
          - **Symbols:** $SYMBOL_COUNT
          - **Total Contracts:** $CONTRACT_COUNT
          - **Data Date:** $DATA_DATE
          - **Last Updated:** $LAST_UPDATED

          ## üöÄ API Endpoints

          ### Base URL
          \`$REPO_URL/\`

          ### Endpoints
          - \`GET /${{ env.SYMBOLS_JSON }}\` - List of all available symbols
          - \`GET /${{ env.INDIV_SYMBOLS_DIR }}/{SYMBOL}.json\` - All contracts for a specific symbol
          - \`GET /${{ env.METADATA_JSON }}\` - Dataset metadata and statistics

          ## üìñ Usage Examples

          \`\`\`bash
          # Get all available symbols
          curl $REPO_URL/${{ env.SYMBOLS_JSON }}

          # Get AAPL options
          curl $REPO_URL/${{ env.INDIV_SYMBOLS_DIR }}/AAPL.json

          # Get metadata
          curl $REPO_URL/${{ env.METADATA_JSON }}
          \`\`\`

          ## üîÑ Update Schedule
          Data is automatically updated every weekday at 3 AM UTC using GitHub Actions.

          ## üìÅ Data Structure
          Each option contract includes:
          - Basic info: symbol, expiration, strike, type (call/put)
          - Pricing: bid, ask
          - Volume and Greeks: volume, delta, gamma, theta, vega, rho

          ## üõ†Ô∏è Technical Details
          - **Data Source:** [${{ vars.DOLT_REPO_URL }}](https://www.dolthub.com/repositories/${{ vars.DOLT_REPO_URL }}) Dolt database
          - **Processing:** TypeScript with GitHub Actions
          - **Hosting:** GitHub Pages (free tier)
          - **Update Frequency:** Weekdays only

          ## ‚ö° Performance
          - Individual symbol files are optimized for size
          - Data is compressed and cached
          - Only active contracts (bid > 0 OR ask > 0) are included

          ## üìù License
          This data is provided for educational and research purposes. Please ensure compliance with your intended usage.

          ---
          *Generated automatically by GitHub Actions*
          EOF

      - name: Optimize Git configuration
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git config --local core.preloadindex true
          git config --local core.fscache true
          git config --local gc.auto 256

      - name: Selective commit and push
        run: |
          # Add files from the API directory and the root README
          git add -A ${{ env.API_DIR }}/
          git add README.md

          # Check if there are any staged changes to commit
          if ! git diff --cached --quiet; then
            # Get file count and metadata for commit message
            CHANGED_FILES=$(git diff --cached --name-only | wc -l)
            METADATA_JSON_PATH="${{ env.API_DIR }}/${{ env.METADATA_JSON }}"
            
            git commit -m "üîÑ Update options data for ${{ needs.extract-data.outputs.actual-date }}
            
            - Files changed: $CHANGED_FILES
            - Symbols: $(jq -r '.symbolCount' ./"$METADATA_JSON_PATH")
            - Contracts: $(jq -r '.totalContracts' ./"$METADATA_JSON_PATH")
            - Data date: $(jq -r '.dataDate' ./"$METADATA_JSON_PATH")"
            
            # Push with optimizations and safety
            git push --force-with-lease
            echo "Changes committed and pushed"
          else
            echo "No changes to commit"
          fi

      - name: Setup Pages
        if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
        uses: actions/configure-pages@v4

      - name: Upload Pages artifact
        if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: './${{ env.API_DIR }}'

      - name: Deploy to GitHub Pages
        if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Cleanup temporary files
        if: always()
        run: |
          rm -rf ${{ env.OUTPUT_DIR }}

  # Job 6: Report final status
  report-status:
    runs-on: ubuntu-latest
    needs: [extract-data, build-typescript, process-data, generate-docs, deploy]
    if: always()

    steps:
      - name: Report workflow status
        run: |
          if [ "${{ needs.deploy.result }}" == "success" ]; then
            echo "‚úÖ Workflow completed successfully"
            echo "üìÖ Data date: ${{ needs.extract-data.outputs.actual-date }}"
            echo "üìä Symbols processed: ${{ needs.process-data.outputs.symbol-count }}"
            echo "üìà Total contracts: ${{ needs.process-data.outputs.contract-count }}"

            # Report database cache status
            if [ "${{ needs.extract-data.outputs.cache-hit }}" == "true" ]; then
              echo "üöÄ Used cached database for faster execution"
            else
              echo "üì• Fresh database clone performed"
            fi

            # Report build optimization status
            if [ "${{ needs.extract-data.outputs.source-changed }}" == "false" ]; then
              echo "‚ö° Skipped TypeScript build (no source changes)"
            elif [ "${{ needs.build-typescript.result }}" == "skipped" ]; then
              echo "‚ö° TypeScript build job skipped"
            else
              echo "üî® TypeScript build executed (source changes detected)"
            fi
            
          else
            echo "‚ùå Workflow failed"
            echo "Check the individual job logs for error details"
            
            # Report which jobs failed
            if [ "${{ needs.extract-data.result }}" == "failure" ]; then
              echo "- Data extraction failed"
            fi
            if [ "${{ needs.build-typescript.result }}" == "failure" ]; then
              echo "- TypeScript build failed"
            fi
            if [ "${{ needs.process-data.result }}" == "failure" ]; then
              echo "- Data processing failed"
            fi
            if [ "${{ needs.generate-docs.result }}" == "failure" ]; then
              echo "- Documentation generation failed"
            fi
            if [ "${{ needs.deploy.result }}" == "failure" ]; then
              echo "- Deployment failed"
            fi
          fi

      - name: Notify on failure
        if: needs.deploy.result == 'failure'
        run: |
          echo "üö® Options data update failed for ${{ needs.extract-data.outputs.actual-date }}"
          echo "Please check the workflow logs for details"
          # Add notification logic here (email, Slack, etc.) if needed
